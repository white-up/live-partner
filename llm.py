from mlx_lm import generate

from actions import strip_action


def sanitize_role_prompt(role_prompt: str) -> str:
    if "scenarios = [" not in role_prompt:
        return role_prompt
    lines = role_prompt.splitlines()
    cleaned_lines: list[str] = []
    skip = False
    for line in lines:
        if line.strip().startswith("scenarios = ["):
            skip = True
            cleaned_lines.append("åŠ¨ä½œç¤ºä¾‹ï¼šä»Žåœºæ™¯ä¸­ä»»é€‰ä¸€å¥è¿›è¡Œæ‹¬å·æå†™ã€‚")
            continue
        if skip and line.strip().startswith("]"):
            skip = False
            continue
        if not skip:
            cleaned_lines.append(line)
    return "\n".join(cleaned_lines).strip()


def build_chat_prompt(
    tokenizer,
    role_prompt: str,
    user_text: str,
    behavior: str,
    history_text: str,
    avoid_text: str,
    stage: str,
) -> str:
    shown_user_text = user_text.strip() or "æ— "
    prompt_content = (
        f"{role_prompt}\n\n"
        "ä½ çŽ°åœ¨è¦è¿›è¡Œä¸€æ¬¡å¼¹çª—äº’åŠ¨å›žåº”ã€‚\n"
        f"å¯¹è¯é˜¶æ®µï¼š{stage}\n"
        f"å½“å‰è¡Œä¸ºï¼š{behavior}\n"
        f"ç”¨æˆ·è¾“å…¥ï¼š{shown_user_text}\n\n"
        f"æœ€è¿‘å¯¹è¯ï¼ˆä¾›å‚è€ƒï¼Œé¿å…é‡å¤ï¼‰ï¼š\n{history_text}\n\n"
        f"ä¸è¦å¤ç”¨ä»¥ä¸‹å¥å­æˆ–è¿‘ä¼¼è¡¨è¾¾ï¼š\n{avoid_text}\n\n"
        "è¦æ±‚ï¼š\n"
        "1) åªè¾“å‡ºä¸€å¥è¯ï¼Œä¸è¦åŠ è§’è‰²åã€ä¸è¦åŠ å‰ç¼€ã€‚\n"
        "2) å¿…é¡»ä½¿ç”¨å°çŒ«è§†è§’ï¼ŒåŒ…å«â€œå¹²å˜›â€æˆ–â€œå¹²å˜›ï¼Ÿâ€ã€‚\n"
        "3) è¾“å‡ºä¸è¶…è¿‡50å­—ï¼ŒçŸ­å¥å£è¯­åŒ–ã€‚\n"
        "5) å¿…é¡»å›žåº”ç”¨æˆ·è¾“å…¥ï¼Œè¯­æ°”è¦è´´åˆå¯¹è¯ã€‚\n"
        "6) è‹¥æ˜¯è¿žç»­å¯¹è¯ï¼Œå¿…é¡»æ˜Žç¡®å›žåº”ç”¨æˆ·çš„è¯ï¼Œæœ€å¥½å¤è¿°ç”¨æˆ·è¾“å…¥ä¸­çš„å…³é”®è¯ã€‚\n"
        "7) ä¸è¦è®²é“ç†ï¼Œä¸è¦ä¸“ä¸šåˆ†æžã€‚\n\n"
        "ç¤ºä¾‹ï¼ˆä»…ä½œæ ¼å¼å‚è€ƒï¼Œç¦æ­¢ç…§æŠ„ï¼‰ï¼š\n"
        "å¹²å˜›å‘€â€¦â€¦æœ¬å–µæŠ±ç€å†°çº¢èŒ¶å‘å‘†å–µ~\n"
        "å¹²å˜›å«æˆ‘ï¼Œæˆ‘åˆç©·åˆç¬¨å˜›~\n\n"
        "è¯·è¾“å‡ºä¸€å¥è¯ä½œä¸ºå¼¹çª—å†…å®¹ã€‚"
    )
    messages = [{"role": "user", "content": prompt_content}]
    return tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )


def generate_reply(
    model,
    tokenizer,
    role_prompt: str,
    user_text: str,
    behavior: str,
    max_tokens: int,
    history_text: str,
    avoid_text: str,
    stage: str,
) -> str:
    prompt = build_chat_prompt(
        tokenizer,
        role_prompt,
        user_text,
        behavior,
        history_text,
        avoid_text,
        stage,
    )
    print("ðŸ§  æç¤ºè¯è¾“å…¥:\n", prompt)
    response = generate(
        model,
        tokenizer,
        prompt=prompt,
        max_tokens=max_tokens,
        verbose=False,
    )
    return clean_reply(response)


def clean_reply(text: str) -> str:
    cleaned = text.strip()
    for token in ("<|im_end|>", "<|im_start|>", "<|endoftext|>"):
        cleaned = cleaned.replace(token, "")
    for prefix in ("cat,", "cat:", "çŒ«,", "çŒ«:", "å¹²å˜›çŒ«:", "å¹²å˜›çŒ«,"):
        if cleaned.lower().startswith(prefix):
            cleaned = cleaned[len(prefix):].strip()
            break
    cleaned = cleaned.replace("`(", "(").replace(")`", ")")
    cleaned = strip_action(cleaned).strip(" \n\r\t\"'")
    if "å¹²å˜›" not in cleaned:
        cleaned = f"å¹²å˜›å‘€â€¦â€¦{cleaned}"
    if len(cleaned) > 50:
        cleaned = cleaned[:50]
    return cleaned
